{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d5b3a9-45c0-458c-b691-a2c0ae39cdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from SplitWindows import SplitWindows\n",
    "from AttentionArea import TemporalAttention , SpatialAttention , DynamicMatrix , AttentionBlock\n",
    "from torch_geometric.nn import GCNConv\n",
    "from Config import CONFIG\n",
    "from Dataset import TimeSDataset\n",
    "import engine\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "Config = CONFIG()\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bec1ab0-1ef0-4ce5-8278-b176448b31d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c6e644-2c51-42e2-a1c2-3f3a6b6038a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write the Main Class\n",
    "\n",
    "class DynDeepNet(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super().__init__()\n",
    "        # Define Clf token\n",
    "        self.CLFToken = torch.rand(1, Config.T_prim, 1)\n",
    "        # Define Temporal Attention\n",
    "        self.TAttention = TemporalAttention (Config.NUM_H ,Config.V * Config.K_E , Config.HIDDEN_DIM)\n",
    "        # Define Spatial Attention \n",
    "        self.SAttention = SpatialAttention()\n",
    "        # Define Dynamic Matrix\n",
    "        self.Dm = DynamicMatrix()\n",
    "        # Define GCN Layer\n",
    "        self.gcn_layer = GCNConv(in_channels=Config.K_E, out_channels=Config.K_F)\n",
    "        # Define Final Encoder layers\n",
    "        self.Transformer = nn.Sequential(*[AttentionBlock(Config.T * Config.K_F, Config.HIDDEN_DIM, Config.NUM_H, Config.DROP_OUT) for _ in range(Config.NUM_LAYER)])\n",
    "        self.Mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(Config.T * Config.K_F),\n",
    "            nn.Linear(Config.T * Config.K_F, Config.NUM_CLASS)\n",
    "            # torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.Dropout = nn.Dropout(Config.DROP_OUT)\n",
    "        self.Flatten = nn.Flatten(2,-1)\n",
    "    def forward(self,x):\n",
    "        ### Input : (B , T_Prim , V)\n",
    "        B , _ , _ = x.shape\n",
    "        ## ADD CLF TOKEN\n",
    "        CLFToken = self.CLFToken.repeat(B, 1 ,1)\n",
    "        x = torch.concatenate([CLFToken, x], axis=-1)\n",
    "        \n",
    "        ## Convert Signal Serie to the Window Signal\n",
    "        x = SplitWindows(x)  ## OutPut : (B , T , V ,P)\n",
    "        ## Temporal Transformer\n",
    "        x = self.TAttention(x)  ## OutPut : (B , T , V , K_E)\n",
    "        ## Spatial Attention\n",
    "        x = self.SAttention(x)    ## OutPut : (B , T , V , K_E)\n",
    "        # Apply Drop out\n",
    "        x = self.Dropout(x)\n",
    "        ## Dynamic Matrix\n",
    "        A = self.Dm(x)           ## OutPut  : (B , T , V , V)\n",
    "        ## GCN Layer\n",
    "        BatchList = []\n",
    "        # Iterate over each graph in the batch\n",
    "        for i in range(B):\n",
    "            # Extract the adjacency matrix for the current graph\n",
    "            adj_i = A[i]  # Shape (T, V, V)\n",
    "            out = []\n",
    "            for t in range(Config.T):\n",
    "                # Get the edge indices and weights for the current time step\n",
    "                edge_index = adj_i[t].nonzero(as_tuple=False).T  # Shape (2, E)\n",
    "                edge_weights = adj_i[t][edge_index[0], edge_index[1]]  # Shape (E,)\n",
    "                # Select the corresponding node features for the current time step\n",
    "                x_t = x[i][t]\n",
    "                # Apply the GCN layer\n",
    "                x_t = self.gcn_layer(x_t, edge_index, edge_weight=edge_weights)\n",
    "                # Store the output for the current time step\n",
    "                out.append(x_t)\n",
    "            BatchList.append(torch.stack(out))\n",
    "        \n",
    "        # Stack the output to get the final representation\n",
    "        x = torch.stack(BatchList)  ## OUTPUT : (B , T , V , K_F) \n",
    "        # Transpose\n",
    "        x = x.transpose(1,2)  ## OUTPUT : (B , V , T , K_F)\n",
    "        # Flatten\n",
    "        x = self.Flatten(x)  ## OUTPUT  : (B , V , T * K_F)\n",
    "        # Drop Out\n",
    "        x = self.Dropout(x)\n",
    "        # Apply Final Encoder\n",
    "        x = x.transpose(0,1)\n",
    "        x = self.Transformer(x)   ## OUTPUT : (V , B , T * K_F)\n",
    "        # Apply Linear Head\n",
    "        cls = x[0]\n",
    "        return self.Mlp_head(cls)\n",
    "\n",
    "Model = DynDeepNet()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d7d41-3800-4c90-80df-1e0384cc996e",
   "metadata": {},
   "source": [
    "## Define Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9a94b8-f35b-4d70-b036-f6a15080f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset = TimeSDataset(\"../Data/AAL/Train\",\"Gender\",label_path=\"../Data/Behavioral-HCP.csv\")\n",
    "TestDataset = TimeSDataset(\"../Data/AAL/Test\",\"Gender\",label_path=\"../Data/Behavioral-HCP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100803ea-79c1-4c43-9393-9adeb9abed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataLoader = DataLoader(TrainDataset ,Config.B ,shuffle = True ,num_workers= 2)\n",
    "TestDataLoader = DataLoader(TestDataset ,Config.B ,shuffle = False ,num_workers= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46efd2-15da-41fd-a32f-06c06e60b00a",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa8c903-31ef-42b6-a441-83ac71ae5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "LR = 0.001\n",
    "Pivot = 80\n",
    "Part = 1\n",
    "Epochs = 20\n",
    "save_weights = False\n",
    "###\n",
    "optimizer = torch.optim.AdamW(Model.parameters() , LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0dd7a5-2c55-439b-8a3e-3a20f3bcd9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 3.80 GiB of which 50.25 MiB is free. Process 7880 has 3.27 GiB memory in use. Of the allocated memory 3.10 GiB is allocated by PyTorch, and 50.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mTrainDataLoader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mTestDataLoader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mPivot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mPart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43msave_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mEpochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/Code/engine.py:175\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, Pivot, Part, save_weights, epochs, device)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 175\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    181\u001b[0m       dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m    182\u001b[0m       loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m    183\u001b[0m       device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n",
      "File \u001b[0;32m/app/Code/engine.py:60\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# 5. Optimizer step\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Calculate and accumulate accuracy metric across all batches\u001b[39;00m\n\u001b[1;32m     63\u001b[0m y_pred_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39msoftmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py:173\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    170\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     adamw(\n\u001b[1;32m    185\u001b[0m         params_with_grad,\n\u001b[1;32m    186\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py:121\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    115\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    116\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    125\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    126\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    127\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 3.80 GiB of which 50.25 MiB is free. Process 7880 has 3.27 GiB memory in use. Of the allocated memory 3.10 GiB is allocated by PyTorch, and 50.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "results = engine.train(Model , \n",
    "                TrainDataLoader,\n",
    "                TestDataLoader,\n",
    "                optimizer,\n",
    "                loss_fn,\n",
    "                Pivot,\n",
    "                Part,\n",
    "                save_weights,\n",
    "                Epochs,\n",
    "                device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb65a5-2abf-4dc6-bb56-4c02fa8d5e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
